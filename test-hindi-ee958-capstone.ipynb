{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11041952,"sourceType":"datasetVersion","datasetId":6874997}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport json\nimport numpy as np\nimport re\nimport string\nimport nltk\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n","metadata":{"id":"BbaaB3gRQYC-","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.588203Z","iopub.execute_input":"2025-03-15T16:34:23.588474Z","iopub.status.idle":"2025-03-15T16:34:23.592982Z","shell.execute_reply.started":"2025-03-15T16:34:23.588454Z","shell.execute_reply":"2025-03-15T16:34:23.592097Z"}},"outputs":[],"execution_count":167},{"cell_type":"code","source":"\n# The 'punkt' resource is a pre-trained model used for tokenization, which is the process of splitting text into individual words or sentences.\n# The 'tab' part likely refers to a variant or extension of the punkt tokenizer that may handle tab-separated data or related formatting nuances.\n# Downloading this resource ensures that the tokenizer is available for use in subsequent NLP tasks.\nnltk.download('punkt_tab')","metadata":{"id":"5G8d1LwtzVJ4","outputId":"ae14cc98-7d70-4544-b64a-6e5ca943e3dd","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.600681Z","iopub.execute_input":"2025-03-15T16:34:23.600908Z","iopub.status.idle":"2025-03-15T16:34:23.608994Z","shell.execute_reply.started":"2025-03-15T16:34:23.600888Z","shell.execute_reply":"2025-03-15T16:34:23.608303Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":168,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":168},{"cell_type":"code","source":"# initialize the variables to process JSON data\nsource_sentences_val = []\n\nid_val = []","metadata":{"id":"wMIkjevT60zr","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.621956Z","iopub.execute_input":"2025-03-15T16:34:23.622176Z","iopub.status.idle":"2025-03-15T16:34:23.631318Z","shell.execute_reply.started":"2025-03-15T16:34:23.622157Z","shell.execute_reply":"2025-03-15T16:34:23.630554Z"}},"outputs":[],"execution_count":170},{"cell_type":"code","source":"with open('/kaggle/input/ee958-cap-val/test_data1_final.json', 'r') as file: # Replace this path with the dataset path in your local machine\n    data = json.load(file)","metadata":{"id":"3D-V5lgY60-u","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.654900Z","iopub.execute_input":"2025-03-15T16:34:23.655143Z","iopub.status.idle":"2025-03-15T16:34:23.766124Z","shell.execute_reply.started":"2025-03-15T16:34:23.655113Z","shell.execute_reply":"2025-03-15T16:34:23.765450Z"}},"outputs":[],"execution_count":173},{"cell_type":"code","source":"# Load souce and target for Validation\nfor language_pair, language_data in data.items():\n    if(language_pair == \"English-Hindi\"):\n      print(f\"Language Pair: {language_pair}\")\n      for data_type, data_entries in language_data.items():\n          print(f\"  Data Type: {data_type}\")\n          for entry_id, entry_data in data_entries.items():\n              source = entry_data[\"source\"]\n              if (data_type == \"Test\"):\n                source_sentences_val.append(source)","metadata":{"id":"eW1JJK4x61HG","outputId":"d020623c-f554-4ac1-dcef-3663c2cf0429","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.767133Z","iopub.execute_input":"2025-03-15T16:34:23.767393Z","iopub.status.idle":"2025-03-15T16:34:23.780195Z","shell.execute_reply.started":"2025-03-15T16:34:23.767363Z","shell.execute_reply":"2025-03-15T16:34:23.779265Z"}},"outputs":[{"name":"stdout","text":"Language Pair: English-Hindi\n  Data Type: Test\n","output_type":"stream"}],"execution_count":174},{"cell_type":"code","source":"print(len(source_sentences_val))","metadata":{"id":"2TXJ4t-K4niH","outputId":"a8bfcfa7-c11b-42d5-c922-7b5be8ea766f","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.781124Z","iopub.execute_input":"2025-03-15T16:34:23.781351Z","iopub.status.idle":"2025-03-15T16:34:23.794683Z","shell.execute_reply.started":"2025-03-15T16:34:23.781331Z","shell.execute_reply":"2025-03-15T16:34:23.793667Z"}},"outputs":[{"name":"stdout","text":"23085\n","output_type":"stream"}],"execution_count":175},{"cell_type":"code","source":"import nltk\nfrom collections import Counter","metadata":{"id":"3XRxnK3y4p6V","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.826189Z","iopub.execute_input":"2025-03-15T16:34:23.826454Z","iopub.status.idle":"2025-03-15T16:34:23.835404Z","shell.execute_reply.started":"2025-03-15T16:34:23.826429Z","shell.execute_reply":"2025-03-15T16:34:23.834655Z"}},"outputs":[],"execution_count":179},{"cell_type":"code","source":"# Function to preprocess and remove punctuation and numbers\ndef preprocess_and_remove_punctuation(sentence):\n    # Remove punctuation and numbers\n    sentence = ''.join([char for char in sentence if char not in string.punctuation and not char.isdigit()])\n    return sentence","metadata":{"id":"K9OBOZcH4p9Q","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.836244Z","iopub.execute_input":"2025-03-15T16:34:23.836482Z","iopub.status.idle":"2025-03-15T16:34:23.844730Z","shell.execute_reply.started":"2025-03-15T16:34:23.836451Z","shell.execute_reply":"2025-03-15T16:34:23.843981Z"}},"outputs":[],"execution_count":180},{"cell_type":"code","source":"# Tokenization and Lowercasing\ndef preprocess(sentences):\n    tokenized_sentences = [nltk.word_tokenize(preprocess_and_remove_punctuation(sentence.lower())) for sentence in sentences]\n    return tokenized_sentences","metadata":{"id":"TIZLuTIH4qCD","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.845380Z","iopub.execute_input":"2025-03-15T16:34:23.845569Z","iopub.status.idle":"2025-03-15T16:34:23.851250Z","shell.execute_reply.started":"2025-03-15T16:34:23.845551Z","shell.execute_reply":"2025-03-15T16:34:23.850665Z"}},"outputs":[],"execution_count":181},{"cell_type":"code","source":"english_test=preprocess(source_sentences_val)","metadata":{"id":"Vlge8LLR4qKn","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:23.862586Z","iopub.execute_input":"2025-03-15T16:34:23.862853Z","iopub.status.idle":"2025-03-15T16:34:26.601453Z","shell.execute_reply.started":"2025-03-15T16:34:23.862829Z","shell.execute_reply":"2025-03-15T16:34:26.600523Z"}},"outputs":[],"execution_count":183},{"cell_type":"code","source":"\nen_test=english_test\n","metadata":{"id":"deSMTBa-5maR","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.604381Z","iopub.execute_input":"2025-03-15T16:34:26.604638Z","iopub.status.idle":"2025-03-15T16:34:26.607850Z","shell.execute_reply.started":"2025-03-15T16:34:26.604606Z","shell.execute_reply":"2025-03-15T16:34:26.607086Z"}},"outputs":[],"execution_count":184},{"cell_type":"code","source":"# torch.cuda.is_available() checks if a CUDA-enabled GPU is available.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"op5h3x5a7lKz","outputId":"3455f8af-d6dc-46bc-b6a0-d5274ddf8e9d","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.633048Z","iopub.execute_input":"2025-03-15T16:34:26.633275Z","iopub.status.idle":"2025-03-15T16:34:26.644936Z","shell.execute_reply.started":"2025-03-15T16:34:26.633254Z","shell.execute_reply":"2025-03-15T16:34:26.644289Z"}},"outputs":[{"execution_count":187,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":187},{"cell_type":"code","source":"# Load vocabularies for English-Hindi\nwith open('/kaggle/input/ee958-cap-val/hi_en_index2word.json', 'r') as f:\n    en_index2word = json.load(f)\nwith open('/kaggle/input/ee958-cap-val/hi_de_index2word.json', 'r') as f:\n    de_index2word = json.load(f)\n\n# Convert to word-to-index mappings\nen_word2index = {token: idx for idx, token in enumerate(en_index2word)}\nde_word2index = {token: idx for idx, token in enumerate(de_index2word)}\n\nprint(\"Vocabularies loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.677124Z","iopub.execute_input":"2025-03-15T16:34:26.677308Z","iopub.status.idle":"2025-03-15T16:34:26.777261Z","shell.execute_reply.started":"2025-03-15T16:34:26.677292Z","shell.execute_reply":"2025-03-15T16:34:26.776519Z"}},"outputs":[{"name":"stdout","text":"Vocabularies loaded successfully\n","output_type":"stream"}],"execution_count":191},{"cell_type":"code","source":"seq_length = 20","metadata":{"id":"bUmeVoIn61Kx","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.778033Z","iopub.execute_input":"2025-03-15T16:34:26.778272Z","iopub.status.idle":"2025-03-15T16:34:26.781671Z","shell.execute_reply.started":"2025-03-15T16:34:26.778243Z","shell.execute_reply":"2025-03-15T16:34:26.780783Z"}},"outputs":[],"execution_count":192},{"cell_type":"code","source":"def encode_and_pad(vocab, sent, max_length):\n    \"\"\"\n    Encodes a sentence using a vocabulary and pads or truncates it to a specified maximum length.\n\n    Args:\n        vocab (dict): A dictionary mapping words to their corresponding indices.\n        sent (list): A list of words representing the sentence to be encoded.\n        max_length (int): The maximum length of the encoded and padded/truncated sentence.\n\n    Returns:\n        list: The encoded and padded/truncated sentence as a list of indices.\n    \"\"\"\n\n    # Define special tokens: Start of Sentence (SOS), End of Sentence (EOS), and Padding (PAD).\n    sos = [vocab[\"<SOS>\"]]\n    eos = [vocab[\"<EOS>\"]]\n    pad = [vocab[\"<PAD>\"]]\n        \n    encoded = [vocab.get(w, vocab[\"<PAD>\"]) for w in sent]\n\n    # Check if the sentence length (excluding SOS and EOS) is less than the maximum length.\n    if len(sent) < max_length - 2: # -2 for SOS and EOS\n        # Calculate the number of padding tokens needed.\n        n_pads = max_length - 2 - len(sent)\n        # Encode the sentence by looking up the index of each word in the vocabulary.\n        #encoded = [vocab[w] for w in sent]\n        return sos + encoded + eos + pad * n_pads\n    else: # sent is longer than max_length; truncating\n        #encoded = [vocab[w] for w in sent]\n        truncated = encoded[:max_length - 2]\n        return sos + truncated + eos\n","metadata":{"id":"hKpPwrsb61Of","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.782405Z","iopub.execute_input":"2025-03-15T16:34:26.782711Z","iopub.status.idle":"2025-03-15T16:34:26.792621Z","shell.execute_reply.started":"2025-03-15T16:34:26.782679Z","shell.execute_reply":"2025-03-15T16:34:26.791895Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"# Encoded Training data\nen_test_encoded = [encode_and_pad(en_word2index, sent, seq_length) for sent in en_test]","metadata":{"id":"zOVE8NxB61SJ","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.793341Z","iopub.execute_input":"2025-03-15T16:34:26.793630Z","iopub.status.idle":"2025-03-15T16:34:26.915064Z","shell.execute_reply.started":"2025-03-15T16:34:26.793580Z","shell.execute_reply":"2025-03-15T16:34:26.914384Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"batch_size = 256\n\ntest_x = np.array(en_test_encoded)\n\ntest_ds = TensorDataset(torch.from_numpy(test_x))\n","metadata":{"id":"zIC407YH9Eif","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.920194Z","iopub.execute_input":"2025-03-15T16:34:26.920504Z","iopub.status.idle":"2025-03-15T16:34:26.973964Z","shell.execute_reply.started":"2025-03-15T16:34:26.920483Z","shell.execute_reply":"2025-03-15T16:34:26.973204Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"import math","metadata":{"id":"IxcBu2_S8HsY","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:26.998071Z","iopub.execute_input":"2025-03-15T16:34:26.998276Z","iopub.status.idle":"2025-03-15T16:34:27.008788Z","shell.execute_reply.started":"2025-03-15T16:34:26.998257Z","shell.execute_reply":"2025-03-15T16:34:27.007995Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"# As suggested in online docs we need to use a positional encoder for Transformaers\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    This module implements positional encoding for transformer models.\n    Written with help of:\n    https://www.geeksforgeeks.org/positional-encoding-in-transformers/\n    https://github.com/hyunwoongko/transformer\n\n    Positional encoding adds information about the position of tokens in a sequence to the input embeddings.\n    This is crucial because transformer models, unlike recurrent neural networks, do not inherently\n    process sequential data in order.\n    \"\"\"\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Initializes the PositionalEncoding module.\n\n        Args:\n            d_model (int): The dimensionality of the input embeddings.\n            max_len (int): The maximum length of the sequences the model can handle.\n        \"\"\"\n        # Create a zero tensor of shape (max_len, d_model) to store the positional encodings.\n        super(PositionalEncoding, self).__init__()\n        # Create a zero tensor of shape (max_len, d_model) to store the positional encodings.\n        pe = torch.zeros(max_len, d_model)\n        # Create a tensor of positions from 0 to max_len-1.\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        # Calculate the division term for the sinusoidal functions.\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        # Calculate the sine and cosine values for even indices and odd indices.\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        # Add a batch dimension to the positional encoding tensor. WHY????? TODO figure out\n        pe = pe.unsqueeze(0)\n        # Buffers are tensors that  are not updated during trainin but are still\n        # saved in the model's state dictionary.\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Applies positional encoding to the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, d_model).\n\n        Returns:\n            torch.Tensor: The input tensor with positional encoding added, of the same shape as x.\n        \"\"\"\n        # The positional encoding is sliced to match the sequence length of the input.\n        return x + self.pe[:, :x.size(1), :]\n","metadata":{"id":"nrjcbW8T4JEY","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.019581Z","iopub.execute_input":"2025-03-15T16:34:27.019797Z","iopub.status.idle":"2025-03-15T16:34:27.029583Z","shell.execute_reply.started":"2025-03-15T16:34:27.019779Z","shell.execute_reply":"2025-03-15T16:34:27.028951Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"# Improved Encoder and Decoder with LSTM and Attention\nclass EnhancedEncoder(nn.Module):\n    \"\"\"\n    An enhanced encoder module that combines embedding, positional encoding, multi-head attention,\n    and feed-forward network for sequence encoding.\n    \"\"\"\n    def __init__(self, input_size, hidden_size, num_heads=4, dropout=0.1):\n        \"\"\"\n        Initializes the EnhancedEncoder module.\n\n        Args:\n            input_size (int): The size of the input vocabulary.\n            hidden_size (int): The dimensionality of the hidden state and embeddings.\n            num_heads (int): The number of attention heads.\n            dropout (float): Dropout probability.\n        \"\"\"\n        super(EnhancedEncoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n\n        # Embedding layer to convert input tokens to embeddings.\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=0)\n        # Positional encoding to add positional information to embeddings.\n        self.pos_encoding = PositionalEncoding(hidden_size)\n        # Multi-head attention layer.\n        self.attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)\n        # Layer normalization after attention.\n        self.norm1 = nn.LayerNorm(hidden_size)\n        # Layer normalization after feed-forward network.\n        self.norm2 = nn.LayerNorm(hidden_size)\n        # Feed-forward network.\n        self.ffn = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size * 4),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size * 4, hidden_size)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden=None):\n        \"\"\"\n        Forward pass of the encoder.\n\n        Args:\n            input (torch.Tensor): Input tensor of shape (batch_size, seq_length).\n            hidden (torch.Tensor, optional): Hidden state (not used in this encoder).\n\n        Returns:\n            tuple: A tuple containing the encoder output and None (for compatibility).\n        \"\"\"\n        # Embed the input tokens. \n        embedded = self.embedding(input)  # [batch_size, seq_length, hidden_size]\n        # Add positional encoding. need to figure out why this is a must\n        embedded = self.pos_encoding(embedded)\n        # Permute dimensions for multi-head attention. \n        # https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n        embedded = embedded.permute(1, 0, 2)  # [seq_length, batch_size, hidden_size]\n\n        # Apply multi-head attention.\n        # https://paperswithcode.com/method/multi-head-attention\n        attn_output, _ = self.attention(embedded, embedded, embedded)\n        attn_output = self.norm1(embedded + self.dropout(attn_output))\n        ffn_output = self.ffn(attn_output)\n        output = self.norm2(attn_output + self.dropout(ffn_output))\n        output = output.permute(1, 0, 2)  # [batch_size, seq_length, hidden_size]\n        return output, None\n\n    def initHidden(self):\n        \"\"\"\n        Initializes the hidden state (not used in this encoder).\n\n        Returns:\n            None\n        \"\"\"\n        return None","metadata":{"id":"1hNA9u659U-E","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.030407Z","iopub.execute_input":"2025-03-15T16:34:27.030587Z","iopub.status.idle":"2025-03-15T16:34:27.044294Z","shell.execute_reply.started":"2025-03-15T16:34:27.030571Z","shell.execute_reply":"2025-03-15T16:34:27.043578Z"}},"outputs":[],"execution_count":202},{"cell_type":"code","source":"class EnhancedDecoder(nn.Module):\n    \"\"\"\n    An enhanced decoder module that combines embedding, positional encoding, self-attention,\n    encoder-decoder attention, and a feed-forward network for sequence decoding.\n    \"\"\"\n    def __init__(self, hidden_size, output_size, num_heads=4, dropout=0.1):\n        \"\"\"\n        Initializes the EnhancedDecoder module.\n\n        Args:\n            hidden_size (int): The dimensionality of the hidden state and embeddings.\n            output_size (int): The size of the output vocabulary.\n            num_heads (int): The number of attention heads.\n            dropout (float): Dropout probability.\n        \"\"\"\n        super(EnhancedDecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n\n        # Embedding layer to convert input tokens to embeddings.\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n        # Positional encoding to add positional information to embeddings.\n        self.pos_encoding = PositionalEncoding(hidden_size)\n        # Self-attention layer.\n        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)\n        # Encoder-decoder attention layer.\n        self.enc_dec_attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)\n        # Layer normalization after self-attention.\n        self.norm1 = nn.LayerNorm(hidden_size)\n        # Layer normalization after encoder-decoder attention.\n        self.norm2 = nn.LayerNorm(hidden_size)\n        # Layer normalization after feed-forward network.\n        self.norm3 = nn.LayerNorm(hidden_size)\n        # Feed-forward network.\n        self.ffn = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size * 4),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size * 4, hidden_size)\n        )\n        # Output linear layer.\n        self.out = nn.Linear(hidden_size, output_size)\n        # Log softmax for output probabilities.\n        self.softmax = nn.LogSoftmax(dim=-1)\n        # Dropout layer.\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, encoder_output, mask=None):\n        \"\"\"\n        Forward pass of the decoder.\n\n        Args:\n            input (torch.Tensor): Input tensor of shape (batch_size, seq_len).\n            encoder_output (torch.Tensor): Output tensor from the encoder of shape (batch_size, src_seq_len, hidden_size).\n            mask (torch.Tensor, optional): Mask for self-attention (e.g., for padding or look-ahead).\n\n        Returns:\n            tuple: A tuple containing the decoder output and None (for compatibility).\n        \"\"\"\n        # Embed the input tokens.\n        embedded = self.embedding(input)  # [batch_size, seq_len, hidden_size]\n        # Add positional encoding.\n        embedded = self.pos_encoding(embedded)\n        # Permute dimensions for multi-head attention.\n        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, hidden_size]\n        # Permute encoder output dimensions for encoder-decoder attention.\n        enc_output = encoder_output.permute(1, 0, 2)  # [src_seq_len, batch_size, hidden_size]\n\n        # Apply self-attention.\n        self_attn_output, _ = self.self_attention(embedded, embedded, embedded, attn_mask=mask)\n        # Apply layer normalization and residual connection after self-attention.\n        self_attn_output = self.norm1(embedded + self.dropout(self_attn_output))\n\n        # Apply encoder-decoder attention.\n        attn_output, _ = self.enc_dec_attention(self_attn_output, enc_output, enc_output)\n        # Apply layer normalization and residual connection after encoder-decoder attention.\n        attn_output = self.norm2(self_attn_output + self.dropout(attn_output))\n\n        # Apply feed-forward network.\n        ffn_output = self.ffn(attn_output)\n        # Apply layer normalization and residual connection after feed-forward network.\n        output = self.norm3(attn_output + self.dropout(ffn_output))\n        # Permute dimensions back to (batch_size, seq_len, hidden_size).\n        output = output.permute(1, 0, 2)  # [batch_size, seq_len, hidden_size]\n\n        # Output logits for the last token only\n        output = self.out(output[:, -1, :])  # [batch_size, output_size]\n        output = self.softmax(output)\n        return output, None\n\n    # We do not need an .initHidden() method for the decoder since the encoder output will act as input in the first decoder time-step","metadata":{"id":"d050fwM09Xad","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.044923Z","iopub.execute_input":"2025-03-15T16:34:27.045114Z","iopub.status.idle":"2025-03-15T16:34:27.055003Z","shell.execute_reply.started":"2025-03-15T16:34:27.045086Z","shell.execute_reply":"2025-03-15T16:34:27.054394Z"}},"outputs":[],"execution_count":203},{"cell_type":"code","source":"# Modified instantiation\nhidden_size = 128  # Increased hidden size for better representation\n\n# Training loop\ncriterion = nn.CrossEntropyLoss()\nSOS = en_word2index[\"<SOS>\"]\nEOS = en_word2index[\"<EOS>\"]","metadata":{"id":"9KKwV9qS9Eyw","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.055839Z","iopub.execute_input":"2025-03-15T16:34:27.056060Z","iopub.status.idle":"2025-03-15T16:34:27.070389Z","shell.execute_reply.started":"2025-03-15T16:34:27.056041Z","shell.execute_reply":"2025-03-15T16:34:27.069646Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf')\"\"\"\n    # Create an upper triangular matrix of ones.\n    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n    # Replace 0s with float('-inf') and 1s with 0.0.\n    # masked_fill(condition, value) replaces elements where the condition is true with the specified value.\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask","metadata":{"id":"5IH25tA04lYz","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.071126Z","iopub.execute_input":"2025-03-15T16:34:27.071354Z","iopub.status.idle":"2025-03-15T16:34:27.082185Z","shell.execute_reply.started":"2025-03-15T16:34:27.071335Z","shell.execute_reply":"2025-03-15T16:34:27.081511Z"}},"outputs":[],"execution_count":205},{"cell_type":"code","source":"# Modified evaluation code\n# Evaluation\n# Corrected Evaluation\ndef evaluate(encoder, decoder, test_ds):\n    \"\"\"\n    Evaluates the encoder and decoder models on the test dataset.\n\n    Args:\n        encoder (nn.Module): The encoder model.\n        decoder (nn.Module): The decoder model.\n        test_ds (Dataset): The test dataset.\n\n    Returns:\n        list: A list of predicted sentences as strings.\n    \"\"\"\n    # Set the models to evaluation mode.\n    encoder.eval()\n    decoder.eval()\n    val_outs = []\n\n    # Disable gradient calculation during evaluation.\n    with torch.no_grad():\n        # Iterate over the test dataset.\n        for i in tqdm(range(len(test_ds))):\n            # Get the input tensor and move it to the device.\n            input_tensor = test_ds[i][0].unsqueeze(0).to(device)\n            encoder_output, _ = encoder(input_tensor)\n            # Initialize the decoder input with the SOS token.\n            decoder_input = torch.tensor([[SOS]], device=device)  # [1, 1]\n            result = []\n\n            # Iterate over the sequence length.\n            for t in range(seq_length):\n                # Generate the mask for the decoder.\n                mask = generate_square_subsequent_mask(t + 1).to(device)\n                # Decode the input sequence up to time step t.\n                decoder_output, _ = decoder(decoder_input, encoder_output, mask)\n                # Get the predicted token index.\n                best = decoder_output.argmax(-1)  # [batch_size], here [1]\n                pred_token = best.item()\n                \n                result.append(de_index2word[pred_token])\n\n                # Check if the predicted token is the EOS token.\n                if pred_token == EOS:\n                    break\n                    \n                \n                # Prepare the next decoder input.\n                # Ensure 2D tensor by concatenating with a tensor containing the pred_token.\n                # Fix: by unsqueezing only once\n                decoder_input = torch.cat(\n                    [decoder_input, torch.tensor([[pred_token]], device=device)],\n                    dim=1\n                )\n\n            # Remove special tokens from the result list.\n            result = [token for token in result if token not in ['<EOS>', '<PAD>', '<SOS>']]\n            # Construct sentence\n            val_outs.append(\" \".join(result))\n    \n    return val_outs","metadata":{"id":"pZBPRQqM5Pfz","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.130044Z","iopub.execute_input":"2025-03-15T16:34:27.130244Z","iopub.status.idle":"2025-03-15T16:34:27.141730Z","shell.execute_reply.started":"2025-03-15T16:34:27.130225Z","shell.execute_reply":"2025-03-15T16:34:27.140927Z"}},"outputs":[],"execution_count":210},{"cell_type":"code","source":"encoder = EnhancedEncoder(len(en_index2word), hidden_size).to(device)\ndecoder = EnhancedDecoder(hidden_size, len(de_index2word)).to(device)\n\n# Load saved state dictionaries with weights_only=True\nencoder.load_state_dict(torch.load('/kaggle/input/ee958-cap-val/encoder_hi.pth', weights_only=True))\ndecoder.load_state_dict(torch.load('/kaggle/input/ee958-cap-val/decoder_hi.pth', weights_only=True))\nprint(\"English-Hindi Encoder and Decoder loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:27.142517Z","iopub.execute_input":"2025-03-15T16:34:27.142745Z","iopub.status.idle":"2025-03-15T16:34:29.197401Z","shell.execute_reply.started":"2025-03-15T16:34:27.142722Z","shell.execute_reply":"2025-03-15T16:34:29.196371Z"}},"outputs":[{"name":"stdout","text":"English-Hindi Encoder and Decoder loaded\n","output_type":"stream"}],"execution_count":211},{"cell_type":"code","source":"# Run evaluation\nval_ids = [i for i, _ in data[\"English-Hindi\"][\"Test\"].items()]\nval_outs = evaluate(encoder, decoder, test_ds)","metadata":{"id":"keeFGS8L5RgB","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:34:29.198386Z","iopub.execute_input":"2025-03-15T16:34:29.198744Z","iopub.status.idle":"2025-03-15T16:44:46.007103Z","shell.execute_reply.started":"2025-03-15T16:34:29.198708Z","shell.execute_reply":"2025-03-15T16:44:46.006390Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 23085/23085 [10:16<00:00, 37.43it/s]\n","output_type":"stream"}],"execution_count":212},{"cell_type":"code","source":"# Save results\ndf0 = pd.DataFrame()\ndf0[\"ID\"] = val_ids\ndf0[\"Translation\"] = val_outs\ndf0.to_csv('/kaggle/working/answersH.csv', index=False)\n#df0.to_csv('/kaggle/working/answersB.csv', index=False)","metadata":{"id":"83naaUJdGthF","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:44:46.007953Z","iopub.execute_input":"2025-03-15T16:44:46.008314Z","iopub.status.idle":"2025-03-15T16:44:46.092374Z","shell.execute_reply.started":"2025-03-15T16:44:46.008268Z","shell.execute_reply":"2025-03-15T16:44:46.091754Z"}},"outputs":[],"execution_count":213},{"cell_type":"code","source":"x=pd.read_csv(\"/kaggle/working/answersH.csv\")","metadata":{"id":"0YPeBfJDGtmL","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:44:46.093161Z","iopub.execute_input":"2025-03-15T16:44:46.093400Z","iopub.status.idle":"2025-03-15T16:44:46.175750Z","shell.execute_reply.started":"2025-03-15T16:44:46.093366Z","shell.execute_reply":"2025-03-15T16:44:46.174839Z"}},"outputs":[],"execution_count":214},{"cell_type":"code","source":"x","metadata":{"id":"A2S8UnGoLKDt","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:44:46.176768Z","iopub.execute_input":"2025-03-15T16:44:46.177090Z","iopub.status.idle":"2025-03-15T16:44:46.197126Z","shell.execute_reply.started":"2025-03-15T16:44:46.177060Z","shell.execute_reply":"2025-03-15T16:44:46.196406Z"}},"outputs":[{"execution_count":215,"output_type":"execute_result","data":{"text/plain":"           ID                                        Translation\n0      540139  और फिर हमें विश्वास दिलाने की आवश्यकता है कि स...\n1      540140  पहली जनवरी के लिए निर्धारित कार्यक्रम निर्धारि...\n2      540141  सन् में वर्ग किलोमीटर क्षेत्र में फैला हुआ है ...\n3      540142  स्थानीय संगीतकार सम्राटों के साथ संगीतकारों पर...\n4      540143  बेशक इस कोर्स के बारे में कुछ और अधिक आसान हो ...\n...       ...                                                ...\n23080  563219  राम को एक ईमेल भेजकर पूछें और उसे कैसे मदद करत...\n23081  563220  श्रीनगर के अंत में भारत सरकार के प्रोफेसर देवस...\n23082  563221  प्रारंभिक रूप से ट्यूबों में पेलेट को असंयमिता...\n23083  563222  दोनों हाथों की हड्‍डी के साथ दायें हाथ को सुचा...\n23084  563223                        हमारा बिल भी अधिक ऊँचा था ।\n\n[23085 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540139</td>\n      <td>और फिर हमें विश्वास दिलाने की आवश्यकता है कि स...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540140</td>\n      <td>पहली जनवरी के लिए निर्धारित कार्यक्रम निर्धारि...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>540141</td>\n      <td>सन् में वर्ग किलोमीटर क्षेत्र में फैला हुआ है ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>540142</td>\n      <td>स्थानीय संगीतकार सम्राटों के साथ संगीतकारों पर...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>540143</td>\n      <td>बेशक इस कोर्स के बारे में कुछ और अधिक आसान हो ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23080</th>\n      <td>563219</td>\n      <td>राम को एक ईमेल भेजकर पूछें और उसे कैसे मदद करत...</td>\n    </tr>\n    <tr>\n      <th>23081</th>\n      <td>563220</td>\n      <td>श्रीनगर के अंत में भारत सरकार के प्रोफेसर देवस...</td>\n    </tr>\n    <tr>\n      <th>23082</th>\n      <td>563221</td>\n      <td>प्रारंभिक रूप से ट्यूबों में पेलेट को असंयमिता...</td>\n    </tr>\n    <tr>\n      <th>23083</th>\n      <td>563222</td>\n      <td>दोनों हाथों की हड्‍डी के साथ दायें हाथ को सुचा...</td>\n    </tr>\n    <tr>\n      <th>23084</th>\n      <td>563223</td>\n      <td>हमारा बिल भी अधिक ऊँचा था ।</td>\n    </tr>\n  </tbody>\n</table>\n<p>23085 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":215}]}